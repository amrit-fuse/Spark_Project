{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/04 09:08:39 WARN Utils: Your hostname, AMRIT resolves to a loopback address: 127.0.1.1; using 172.29.233.69 instead (on interface eth0)\n",
      "22/11/04 09:08:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/04 09:08:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from All_details import *\n",
    "\n",
    "from pyspark.sql import SparkSession , functions as F\n",
    "\n",
    "from pyspark.sql.functions import udf,col,countDistinct,date_format,row_number\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "spark = SparkSession.builder.appName('Crimes_in_Boston')\\\n",
    "    .config('spark.driver.extraClassPath', '/usr/lib/jvm/java-17-openjdk-amd64/lib/postgresql-42.5.0.jar')\\\n",
    "    .getOrCreate()\n",
    "################################## jar file path maybe different for you ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load csv into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crimes_df = spark.read.csv('DATA/crimes.csv', header=True, inferSchema=True)\n",
    "offense_codes_df = spark.read.csv('DATA/offense_codes.csv', header=True, inferSchema=True)\n",
    "police_district_codes_df = spark.read.csv('DATA/police_district_codes.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if dataset  imported correctly and Dataframe is not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_empty_df(df):\n",
    "    if df.count() == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_empty_df(crimes_df) == False , 'crimes_df is empty'\n",
    "assert test_empty_df(offense_codes_df) == False , 'offense_codes_df is empty'\n",
    "assert test_empty_df(police_district_codes_df) == False , 'police_district_codes_df is empty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to test Schema and  rows count of data in databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_database(df, table_name):\n",
    "    if spark.read.jdbc(url=URL, table=table_name, properties=Properties).schema == df.schema and spark.read.jdbc(url=URL, table=table_name, properties=Properties).count() == df.count():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "perform all preprocessing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|     STREET|        Lat|        Long|            Location|\n",
      "+---------------+------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|     I182070945|         619|             Larceny|     D14|           808|       N|2018-09-02 13:00:00|2018|    9|     Sunday|  13|  Part One| LINCOLN ST|42.35779134|-71.13937053|(42.35779134, -71...|\n",
      "|     I182070943|        1402|           Vandalism|     C11|           347|       N|2018-08-21 00:00:00|2018|    8|    Tuesday|   0|  Part Two|   HECLA ST|42.30682138|-71.06030035|(42.30682138, -71...|\n",
      "|     I182070941|        3410|               Towed|      D4|           151|       N|2018-09-03 19:27:00|2018|    9|     Monday|  19|Part Three|CAZENOVE ST|42.34658879|-71.07242943|(42.34658879, -71...|\n",
      "|     I182070940|        3114|Investigate Property|      D4|           272|       N|2018-09-03 21:16:00|2018|    9|     Monday|  21|Part Three| NEWCOMB ST|42.33418175|-71.07866441|(42.33418175, -71...|\n",
      "|     I182070938|        3114|Investigate Property|      B3|           421|       N|2018-09-03 21:05:00|2018|    9|     Monday|  21|Part Three|   DELHI ST|42.27536542|-71.09036101|(42.27536542, -71...|\n",
      "+---------------+------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+--------------------+\n",
      "|CODE|                NAME|\n",
      "+----+--------------------+\n",
      "| 111|MURDER, NON-NEGLI...|\n",
      "| 112|KILLING OF FELON ...|\n",
      "| 113|KILLING OF FELON ...|\n",
      "| 114|KILLING OF POLICE...|\n",
      "| 121|MANSLAUGHTER - VE...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for SHOOTING column replace null values with 'N'\n",
    "crimes_df = crimes_df.withColumn('SHOOTING', F.when(F.col('SHOOTING').isNull(), 'N')\\\n",
    "    .otherwise(F.col('SHOOTING')))\n",
    "\n",
    "# remove OFFENSE_DESCRIPTION column\n",
    "crimes_df = crimes_df.drop('OFFENSE_DESCRIPTION')\n",
    "crimes_df.show(5)\n",
    "\n",
    "# keep only first duplicate value in offence_code_df\n",
    "offense_codes_df = offense_codes_df.dropDuplicates(['CODE'])\n",
    "offense_codes_df.sort ('CODE').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find all the list of dates in 2017 where ‘VANDALISM’ happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|   OCCURRED_ON_DATE|     Name|\n",
      "+-------------------+---------+\n",
      "|2017-08-01 00:00:00|VANDALISM|\n",
      "|2017-07-01 08:00:00|VANDALISM|\n",
      "|2017-12-15 15:02:00|VANDALISM|\n",
      "|2017-10-31 20:18:00|VANDALISM|\n",
      "|2017-02-12 12:00:00|VANDALISM|\n",
      "|2017-12-30 16:00:00|VANDALISM|\n",
      "|2017-12-30 20:00:00|VANDALISM|\n",
      "|2017-12-31 12:00:00|VANDALISM|\n",
      "|2017-11-22 12:00:00|VANDALISM|\n",
      "|2017-12-24 08:00:00|VANDALISM|\n",
      "|2017-12-31 18:00:00|VANDALISM|\n",
      "|2017-12-31 10:00:00|VANDALISM|\n",
      "|2017-12-31 20:55:00|VANDALISM|\n",
      "|2017-12-28 21:00:00|VANDALISM|\n",
      "|2017-12-30 19:00:00|VANDALISM|\n",
      "|2017-12-30 23:00:00|VANDALISM|\n",
      "|2017-12-31 12:57:00|VANDALISM|\n",
      "|2017-12-30 20:00:00|VANDALISM|\n",
      "|2017-12-31 12:39:00|VANDALISM|\n",
      "|2017-12-31 11:10:00|VANDALISM|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1. Find all the list of dates in 2017 where ‘VANDALISM’ happened.\n",
    "\n",
    "join_offense_code = crimes_df.join(offense_codes_df,crimes_df.OFFENSE_CODE==offense_codes_df.CODE,\"inner\")\n",
    "vandalism_2017 = join_offense_code.filter((offense_codes_df['Name']=='VANDALISM' ) & (crimes_df['Year']==2017)).select(crimes_df['OCCURRED_ON_DATE'],offense_codes_df['Name'])\n",
    "vandalism_2017.show()\n",
    "\n",
    "############### SAVE to POSTGRES ###############\n",
    "vandalism_2017.write.jdbc(url=URL, table='vandalism_2017', mode='overwrite', properties=Properties)\n",
    "\n",
    "##############  TEST  ##############\n",
    "assert test_database(vandalism_2017, 'vandalism_2017') == False, 'vandalism_2017 is having different schema or count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Show the data frame where the District is  null and then fill the null District with “District not Verified”. (udf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|            DISTRICT|\n",
      "+---------------+------------+--------------------+--------------------+\n",
      "|     I182070920|        3006|  Medical Assistance|District Not Verifed|\n",
      "|     I182070913|        3006|  Medical Assistance|District Not Verifed|\n",
      "|     I182070906|        3831|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070889|        1843|      Drug Violation|District Not Verifed|\n",
      "|     I182070889|        3125|     Warrant Arrests|District Not Verifed|\n",
      "|     I182070889|        1841|      Drug Violation|District Not Verifed|\n",
      "|     I182070866|        3807|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070865|         301|             Robbery|District Not Verifed|\n",
      "|     I182070839|        3820|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070836|        3803|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070820|        1806|      Drug Violation|District Not Verifed|\n",
      "|     I182070799|        3410|               Towed|District Not Verifed|\n",
      "|     I182070724|        3801|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070718|         802|      Simple Assault|District Not Verifed|\n",
      "|     I182070677|        2905|          Violations|District Not Verifed|\n",
      "|     I182070677|        3802|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070673|        3803|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070646|        3116|Harbor Related In...|District Not Verifed|\n",
      "|     I182070630|        3831|Motor Vehicle Acc...|District Not Verifed|\n",
      "|     I182070624|        2405|  Disorderly Conduct|District Not Verifed|\n",
      "+---------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.Show the data frame where the District is  null and then fill the null District with “District not Verified”. (udf) \n",
    "def remove_na(replacenull):\n",
    "    return \"District Not Verifed\"\n",
    "udf_name = udf(remove_na)\n",
    "\n",
    "null_district = crimes_df.filter(crimes_df['DISTRICT'].isNull())\n",
    "# null_district.show()\n",
    "\n",
    "fill_na = null_district.select(crimes_df['INCIDENT_NUMBER'],crimes_df['OFFENSE_CODE'],crimes_df['OFFENSE_CODE_GROUP'],udf_name(null_district['DISTRICT']))\\\n",
    ".withColumnRenamed('remove_na(DISTRICT)','DISTRICT')\n",
    "\n",
    "fill_na.show()\n",
    "\n",
    "################### SAVE to POSTGRES ###################\n",
    "fill_na.write.jdbc(url=URL, table='fill_na', mode='overwrite', properties=Properties)\n",
    "\n",
    "##################  TEST  ##################\n",
    "\n",
    "assert test_database(fill_na, 'fill_na') == False, 'fill_na is having different schema or count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.Show the year and total number of Robbery happens in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|YEAR|Total Robbery in Year|\n",
      "+----+---------------------+\n",
      "|2015|                  948|\n",
      "|2016|                 1506|\n",
      "|2017|                 1376|\n",
      "|2018|                  794|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.Show the year and total number of Robbery happens in each year.\n",
    "filtering_robbery = crimes_df.filter((crimes_df.OFFENSE_CODE_GROUP==\"Robbery\")).select(crimes_df.YEAR,crimes_df.OFFENSE_CODE_GROUP)\n",
    "total_robbery = filtering_robbery.groupBy(\"YEAR\").count().orderBy(\"Year\").withColumnRenamed(\"count\",\"Total Robbery in Year\")\n",
    "total_robbery.show()\n",
    "\n",
    "\n",
    "################### SAVE to POSTGRES ###################\n",
    "total_robbery.write.jdbc(url=URL, table='total_robbery', mode='overwrite', properties=Properties)\n",
    "\n",
    "##################  TEST  ##################\n",
    "\n",
    "assert test_database(total_robbery, 'total_robbery') == False, 'total_robbery is having different schema or count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Show all Offense_codes and names which are not listed in crime.csv but in offense_code.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|CODE|                NAME|\n",
      "+----+--------------------+\n",
      "| 113|KILLING OF FELON ...|\n",
      "| 114|KILLING OF POLICE...|\n",
      "| 122|MANSLAUGHTER - TR...|\n",
      "| 124|MANSLAUGHTER - VE...|\n",
      "| 125|MANSLAUGHTER - TR...|\n",
      "| 211|RAPE - FEMALE - F...|\n",
      "| 212| RAPE - MALE - FORCE|\n",
      "| 213|RAPE - FEMALE/MAL...|\n",
      "| 222|RAPE - FEMALE ATT...|\n",
      "| 223|RAPE - MALE - ATT...|\n",
      "| 224|RAPE - FEMALE/MAL...|\n",
      "| 230|RAPE - FEMALE - A...|\n",
      "| 231|RAPE - MALE - ATT...|\n",
      "| 232|RAPE - FEMALE FOR...|\n",
      "| 233|RAPE - FEMALE - F...|\n",
      "| 234|RAPE - MALE - FOR...|\n",
      "| 235|RAPE - MALE - FOR...|\n",
      "| 236|RAPE - FEMALE - D...|\n",
      "| 237|RAPE - MALE - DRU...|\n",
      "| 241|RAPE - ATTEMPT - ...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.Show all Offense_codes and names which are not listed in crime.csv but in offense_code.csv.\n",
    "\n",
    "missing_offences = offense_codes_df.join(crimes_df,offense_codes_df.CODE==crimes_df.OFFENSE_CODE,\"left_anti\")\n",
    "\n",
    "missing_offences.show()\n",
    "\n",
    "################### SAVE to POSTGRES ###################\n",
    "missing_offences.write.jdbc(url=URL, table='missing_offences', mode='overwrite', properties=Properties)\n",
    "\n",
    "##################  TEST  ##################\n",
    "\n",
    "assert test_database(missing_offences, 'missing_offences') == False, 'missing_offences is having different schema or count'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.List Offence Description ( Name) which is occurred on Sunday around time ‘21:30:00’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                NAME|\n",
      "+--------------------+\n",
      "|INVESTIGATE PROPERTY|\n",
      "|M/V - LEAVING SCE...|\n",
      "|    ROBBERY - STREET|\n",
      "|LARCENY THEFT FRO...|\n",
      "|M/V - LEAVING SCE...|\n",
      "|LARCENY THEFT FRO...|\n",
      "|  PROPERTY - MISSING|\n",
      "|M/V ACCIDENT - PR...|\n",
      "|M/V - LEAVING SCE...|\n",
      "|M/V - LEAVING SCE...|\n",
      "|    ROBBERY - STREET|\n",
      "|INVESTIGATE PROPERTY|\n",
      "|M/V - LEAVING SCE...|\n",
      "|        SUDDEN DEATH|\n",
      "|LARCENY THEFT FRO...|\n",
      "|M/V ACCIDENT - PR...|\n",
      "|M/V ACCIDENT INVO...|\n",
      "|VIOL. OF RESTRAIN...|\n",
      "|           VANDALISM|\n",
      "|LARCENY THEFT FRO...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.List offense_description which is occurred on Sunday around time ‘21:30:00’\n",
    "\n",
    "merge_df = crimes_df.join(offense_codes_df,crimes_df.OFFENSE_CODE==offense_codes_df.CODE,\"inner\")\n",
    "\n",
    "sunday_2130 = merge_df.filter((merge_df['DAY_OF_WEEK']=='Sunday') & (merge_df['OCCURRED_ON_DATE'].contains('21:30:00'))).select(merge_df['NAME'])\n",
    "\n",
    "sunday_2130.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################### SAVE to POSTGRES ###################\n",
    "sunday_2130.write.jdbc(url=URL, table='sunday_2130', mode='overwrite', properties=Properties)\n",
    "\n",
    "##################  TEST  ##################\n",
    "\n",
    "assert test_database(sunday_2130, 'sunday_2130') == False, 'sunday_2130 is having different schema or count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (window function) Partition by district , order by  year and then count the offenses including rolling count for each district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------------+-----------------------+\n",
      "|DISTRICT|YEAR|count_incidents|rolling_count_incidents|\n",
      "+--------+----+---------------+-----------------------+\n",
      "|      A1|2015|           6015|                   6015|\n",
      "|      A1|2016|          10923|                  16938|\n",
      "|      A1|2017|          11375|                  28313|\n",
      "|      A1|2018|           7404|                  35717|\n",
      "|     A15|2015|           1027|                   1027|\n",
      "|     A15|2016|           1986|                   3013|\n",
      "|     A15|2017|           2167|                   5180|\n",
      "|     A15|2018|           1325|                   6505|\n",
      "|      A7|2015|           2426|                   2426|\n",
      "|      A7|2016|           4130|                   6556|\n",
      "|      A7|2017|           4264|                  10820|\n",
      "|      A7|2018|           2724|                  13544|\n",
      "|      B2|2015|           8687|                   8687|\n",
      "|      B2|2016|          15706|                  24393|\n",
      "|      B2|2017|          15680|                  40073|\n",
      "|      B2|2018|           9872|                  49945|\n",
      "|      B3|2015|           5617|                   5617|\n",
      "|      B3|2016|          11145|                  16762|\n",
      "|      B3|2017|          11195|                  27957|\n",
      "|      B3|2018|           7485|                  35442|\n",
      "+--------+----+---------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 6. (window function) Partition by district , order by  year and then rolling count the offenses\n",
    "\n",
    "window = Window.partitionBy('DISTRICT').orderBy('YEAR')\n",
    "\n",
    "window_df = crimes_df.filter(F.col('DISTRICT').isNotNull()) # remove null values\n",
    "\n",
    "window_df = window_df.groupBy('DISTRICT', 'YEAR').agg(F.count('INCIDENT_NUMBER').alias('count_incidents'))\n",
    "\n",
    "window_df = window_df.withColumn('rolling_count_incidents', F.sum('count_incidents').over(window))\n",
    "\n",
    "window_df.show()\n",
    "\n",
    "\n",
    "############### SAVE to POSTGRES ###############\n",
    "window_df.write.jdbc(url=URL, table='rolling_count', mode='overwrite', properties=Properties)\n",
    "\n",
    "\n",
    "############### TEST ###############\n",
    "assert test_database(window_df, 'rolling_count') == False, 'rolling_count table is having different schema or count'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pivot incident years and count incident monthwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----+----+\n",
      "|MONTH|2015|2016|2017|2018|\n",
      "+-----+----+----+----+----+\n",
      "|    1|null|7835|7993|7782|\n",
      "|    2|null|7308|7408|6937|\n",
      "|    3|null|8199|8179|7768|\n",
      "|    4|null|8101|8069|7916|\n",
      "|    5|null|8578|8715|8906|\n",
      "|    6|4191|8558|8985|8834|\n",
      "|    7|8324|8619|9075|8538|\n",
      "|    8|8342|8938|9206|8337|\n",
      "|    9|8414|8522|8940| 667|\n",
      "|   10|8308|8583|8846|null|\n",
      "|   11|7818|7922|7935|null|\n",
      "|   12|7991|7951|7535|null|\n",
      "+-----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 7. Pivot incident years and count incident monthwise\n",
    "\n",
    "pivot_df = crimes_df.groupBy('MONTH').pivot('YEAR').count().sort('MONTH')\n",
    "pivot_df.show()\n",
    "\n",
    "############################## SAVE to POSTGRES ###############\n",
    "pivot_df.write.jdbc(url=URL, table='pivot_YEAR', mode='overwrite', properties=Properties)\n",
    "\n",
    "################# TEST ################\n",
    "assert test_database(pivot_df, 'pivot_YEAR') == False, 'pivot_YEAR table is having different schema or count'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Count crimes involving any kind of Robbery in   each district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|DISTRICT|District_Name|count|\n",
      "+--------+-------------+-----+\n",
      "|      A1|     Downtown|  659|\n",
      "|     A15|  Charlestown|   84|\n",
      "|      A7|  East Boston|  232|\n",
      "|      B2|      Roxbury|  860|\n",
      "|      B3|     Mattapan|  530|\n",
      "|     C11|   Dorchester|  660|\n",
      "|      C6| South Boston|  263|\n",
      "|     D14|     Brighton|  175|\n",
      "|      D4|    South End|  626|\n",
      "|     E13|Jamaica Plain|  250|\n",
      "|     E18|    Hyde Park|  170|\n",
      "|      E5| West Roxbury|   96|\n",
      "+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 8. Count crimes involving any kind of \"Robbery\" in  each district name wise. district name is in police_district_codes_df\n",
    "\n",
    "robbery_df = crimes_df.filter(F.col('OFFENSE_CODE_GROUP')== 'Robbery')\n",
    "\n",
    "robbery_df = robbery_df.join(police_district_codes_df, robbery_df.DISTRICT == police_district_codes_df.District_Code, how='left')\n",
    "\n",
    "robbery_df = robbery_df.filter(F.col('DISTRICT').isNotNull()) # remove null values\n",
    "\n",
    "robbery_df = robbery_df.groupBy('DISTRICT', 'District_Name').count().sort('DISTRICT')\n",
    "\n",
    "robbery_df.show()\n",
    "\n",
    "##################SAVE to POSTGRES ################\n",
    "robbery_df.write.jdbc(url=URL, table='robbery_in_each_district', mode='overwrite', properties=Properties)\n",
    "\n",
    "################# TEST #################\n",
    "assert test_database(robbery_df, 'robbery_in_each_district') == False, 'robbery_in_each_district table is having different schema or count'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. For each day, list the hour when the incident number is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+\n",
      "|DAY_OF_WEEK|HOUR|count|\n",
      "+-----------+----+-----+\n",
      "|     Friday|  17| 3252|\n",
      "|     Monday|  17| 3254|\n",
      "|   Saturday|   0| 2612|\n",
      "|     Sunday|   0| 2400|\n",
      "|   Thursday|  18| 3033|\n",
      "|    Tuesday|  17| 3241|\n",
      "|  Wednesday|  17| 3153|\n",
      "+-----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 9. For each day, list the hour when the incident number is highest alonmg with the count of incidents\n",
    "\n",
    "incident_df = crimes_df.groupBy('DAY_OF_WEEK', 'HOUR').count().sort('DAY_OF_WEEK', 'HOUR')\n",
    "\n",
    "incident_df = incident_df.withColumn('max_incident', F.max('count').over(Window.partitionBy('DAY_OF_WEEK')))    \n",
    "\n",
    "incident_df = incident_df.filter(F.col('count') == F.col('max_incident')) \n",
    "\n",
    "incident_df = incident_df.drop('max_incident')\n",
    "\n",
    "incident_df.show()\n",
    "\n",
    "################ SAVE to POSTGRES #################\n",
    "incident_df.write.jdbc(url=URL, table='Day_incident_hour', mode='overwrite', properties=Properties)\n",
    "\n",
    "################# TEST ##################\n",
    "assert test_database(incident_df, 'Day_incident_hour') == False, 'Day_incident_hour table is having different schema or count'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. List highest crime/offense group in each district (name) and the number of incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+-----+\n",
      "|DISTRICT|District_Name|  OFFENSE_CODE_GROUP|count|\n",
      "+--------+-------------+--------------------+-----+\n",
      "|      A1|     Downtown|             Larceny| 4704|\n",
      "|     A15|  Charlestown|Motor Vehicle Acc...|  960|\n",
      "|      A7|  East Boston|Motor Vehicle Acc...| 1516|\n",
      "|      B2|      Roxbury|Motor Vehicle Acc...| 6407|\n",
      "|      B3|     Mattapan|Motor Vehicle Acc...| 3836|\n",
      "|     C11|   Dorchester|Motor Vehicle Acc...| 5305|\n",
      "|      C6| South Boston|Motor Vehicle Acc...| 2699|\n",
      "|     D14|     Brighton|Motor Vehicle Acc...| 2857|\n",
      "|      D4|    South End|             Larceny| 7313|\n",
      "|     E13|Jamaica Plain|Motor Vehicle Acc...| 2166|\n",
      "|     E18|    Hyde Park|Motor Vehicle Acc...| 2366|\n",
      "|      E5| West Roxbury|Motor Vehicle Acc...| 1813|\n",
      "+--------+-------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 10. List highest crime/offense group in each district (name) and the number of incidents\n",
    "\n",
    "district_crime_count = crimes_df.join(police_district_codes_df, crimes_df.DISTRICT == police_district_codes_df.District_Code, how='left')\n",
    "\n",
    "district_crime_count = district_crime_count.filter(F.col('DISTRICT').isNotNull()) # remove null values\n",
    "\n",
    "district_crime_count = district_crime_count.groupBy('DISTRICT', 'District_Name', 'OFFENSE_CODE_GROUP').count().sort('DISTRICT', 'count', ascending=False)\n",
    "\n",
    "district_crime_count = district_crime_count.dropDuplicates(['DISTRICT']) # keep only first duplicate value \n",
    "# as we sorted the dataframe in descending order of count column so first value will be highest\n",
    "\n",
    "district_crime_count.show()\n",
    "\n",
    "################### SAVE to POSTGRES #######################\n",
    "district_crime_count.write.jdbc(url=URL, table='district_crime_count', mode='overwrite', properties=Properties)\n",
    "\n",
    "#################### TEST ###############################\n",
    "assert test_database(district_crime_count, 'district_crime_count') == False, 'district_crime_count table is having different schema or count'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. As each degreee of longitude is 100 km apart,  list crimes with counts (yearwsie)  within a 111km radius of BOSTON police headquater which is at 42.33397849555639, -71.09079628933894 (lat, long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+----+----+\n",
      "|  OFFENSE_CODE_GROUP|2015|2016|2017|2018|\n",
      "+--------------------+----+----+----+----+\n",
      "|  Aggravated Assault|1317|2177|2241|1532|\n",
      "|            Aircraft|   4|   4|  17|   5|\n",
      "|               Arson|  11|  33|  31|  16|\n",
      "|Assembly or Gathe...| 236| 313| 233| 130|\n",
      "|          Auto Theft| 934|1409|1316| 870|\n",
      "| Auto Theft Recovery| 129| 288| 346| 214|\n",
      "|          Ballistics| 159| 284| 327| 166|\n",
      "|   Biological Threat|null|null|   2|null|\n",
      "|           Bomb Hoax|  16|  36|  10|   9|\n",
      "|Burglary - No Pro...|   1|   1|null|null|\n",
      "| Commercial Burglary| 253| 427| 427| 219|\n",
      "|    Confidence Games| 650|1051| 867| 512|\n",
      "|      Counterfeiting| 271| 481| 452| 223|\n",
      "| Criminal Harassment|  42|  35|  28|  26|\n",
      "|  Disorderly Conduct| 451| 741| 788| 420|\n",
      "|      Drug Violation|3060|4651|4115|2770|\n",
      "|        Embezzlement|  61|  84| 108|  42|\n",
      "|        Evading Fare|  78| 124| 110|  77|\n",
      "|          Explosives|   6|   7|   5|   6|\n",
      "|Fire Related Reports| 300| 594| 573| 389|\n",
      "+--------------------+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 16. As each degreee of longitude is 111km apart,  list crimes with counts (yearwsie)  within a 100 km radius of BOSTON police headquater which is at 42.33397849555639, -71.09079628933894 (lat, long) \n",
    "\n",
    "#udf to calculate distance between two \n",
    "def degree_distance(lat1, long1, lat2, long2):\n",
    "    return 111 * F.sqrt(F.pow(lat1 - lat2, 2) + F.pow(long1 - long2, 2)) # return distance in km \n",
    "\n",
    "\n",
    "F.udf(degree_distance, FloatType())\n",
    "\n",
    "crimes_radius_111_df = crimes_df.withColumn('Distance_Apart', degree_distance(F.col('Lat'), F.col('Long'), 42.33397849555639, -71.09079628933894))\n",
    "\n",
    "crimes_radius_111_df = crimes_radius_111_df.filter(F.col('Distance_Apart') <= 111)\n",
    "\n",
    "crimes_radius_111_df=crimes_radius_111_df.groupBy('OFFENSE_CODE_GROUP').pivot('YEAR').count().sort('OFFENSE_CODE_GROUP')\n",
    "\n",
    "crimes_radius_111_df.show()\n",
    "\n",
    "\n",
    "################### SAVE to POSTGRES #######################\n",
    "crimes_radius_111_df.write.jdbc(url=URL, table='crimes_year_radius', mode='overwrite', properties=Properties)\n",
    "\n",
    "#################### TEST ###############################\n",
    "assert test_database(crimes_radius_111_df, 'crimes_year_radius') == False, 'crimes_year_radius table is having different schema or count'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.  List all crimes that occurred in all district (namewsie) and in the  August 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|District_Name|         CRIME_GROUP|\n",
      "+-------------+--------------------+\n",
      "|     Brighton|[Fraud, Auto Thef...|\n",
      "|  Charlestown|[Fraud, Auto Thef...|\n",
      "|   Dorchester|[Fraud, Auto Thef...|\n",
      "|     Downtown|[Fraud, Auto Thef...|\n",
      "|  East Boston|[Fraud, Auto Thef...|\n",
      "|    Hyde Park|[Fraud, Auto Thef...|\n",
      "|Jamaica Plain|[Fraud, Auto Thef...|\n",
      "|     Mattapan|[Fraud, Auto Thef...|\n",
      "|      Roxbury|[Fraud, Auto Thef...|\n",
      "| South Boston|[Fraud, Auto Thef...|\n",
      "|    South End|[Fraud, Auto Thef...|\n",
      "| West Roxbury|[Fraud, Auto Thef...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## 17.  List all crimes that occurred in all district (namewsie) and in the  August 2016 \n",
    "\n",
    "crimes_august_2016_df = crimes_df.filter(F.col('YEAR') == 2016).filter(F.col('MONTH') == 8)\n",
    "\n",
    "crimes_august_2016_df = crimes_august_2016_df.join(police_district_codes_df, crimes_august_2016_df.DISTRICT == police_district_codes_df.District_Code, how='left')\n",
    "\n",
    "crimes_august_2016_df = crimes_august_2016_df.filter(F.col('DISTRICT').isNotNull()) # remove null values\n",
    "\n",
    "\n",
    "crimes_august_2016_df = crimes_august_2016_df.groupBy('District_Name').agg(F.collect_set('OFFENSE_CODE_GROUP').alias('CRIME_GROUP')).sort('DISTRICT_NAME')\n",
    "\n",
    "crimes_august_2016_df.show()\n",
    "\n",
    "################### SAVE to POSTGRES #######################\n",
    "crimes_august_2016_df.write.jdbc(url=URL, table='list_crimes_aug_2016', mode='overwrite', properties=Properties)\n",
    "\n",
    "\n",
    "#################### TEST ###############################\n",
    "assert test_database(crimes_august_2016_df , 'list_crimes_aug_2016') == False, 'list_crimes_aug_2016 table is having different schema or count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spark_project_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c56a022bae3e6673f64e7513c309b6c71ec5c3972dcbf7b2ef6b0a8ca8eda9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
