{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit your info here\n",
    "\n",
    "Properties={'user': 'amrit', 'password': '1234'}\n",
    "URL='jdbc:postgresql://localhost:5432/crimes_in_boston'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession , functions as F\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName('Crimes_in_Boston')\\\n",
    "    .config('spark.driver.extraClassPath', '/usr/lib/jvm/java-17-openjdk-amd64/lib/postgresql-42.5.0.jar')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.csv('DATA/crimes.csv', header=True, inferSchema=True)\n",
    "offense_codes_df = spark.read.csv('DATA/offense_codes.csv', header=True, inferSchema=True)\n",
    "police_district_codes_df = spark.read.csv('DATA/police_district_codes.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "perform all preprocessing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|     STREET|        Lat|        Long|            Location|\n",
      "+---------------+------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|     I182070945|         619|             Larceny|     D14|           808|       N|2018-09-02 13:00:00|2018|    9|     Sunday|  13|  Part One| LINCOLN ST|42.35779134|-71.13937053|(42.35779134, -71...|\n",
      "|     I182070943|        1402|           Vandalism|     C11|           347|       N|2018-08-21 00:00:00|2018|    8|    Tuesday|   0|  Part Two|   HECLA ST|42.30682138|-71.06030035|(42.30682138, -71...|\n",
      "|     I182070941|        3410|               Towed|      D4|           151|       N|2018-09-03 19:27:00|2018|    9|     Monday|  19|Part Three|CAZENOVE ST|42.34658879|-71.07242943|(42.34658879, -71...|\n",
      "|     I182070940|        3114|Investigate Property|      D4|           272|       N|2018-09-03 21:16:00|2018|    9|     Monday|  21|Part Three| NEWCOMB ST|42.33418175|-71.07866441|(42.33418175, -71...|\n",
      "|     I182070938|        3114|Investigate Property|      B3|           421|       N|2018-09-03 21:05:00|2018|    9|     Monday|  21|Part Three|   DELHI ST|42.27536542|-71.09036101|(42.27536542, -71...|\n",
      "+---------------+------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+--------------------+\n",
      "|CODE|                NAME|\n",
      "+----+--------------------+\n",
      "| 111|MURDER, NON-NEGLI...|\n",
      "| 112|KILLING OF FELON ...|\n",
      "| 113|KILLING OF FELON ...|\n",
      "| 114|KILLING OF POLICE...|\n",
      "| 121|MANSLAUGHTER - VE...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for SHOOTING column replace null values with 'N'\n",
    "crimes_df = crimes_df.withColumn('SHOOTING', F.when(F.col('SHOOTING').isNull(), 'N')\\\n",
    "    .otherwise(F.col('SHOOTING')))\n",
    "\n",
    "# remove OFFENSE_DESCRIPTION column\n",
    "crimes_df = crimes_df.drop('OFFENSE_DESCRIPTION')\n",
    "\n",
    "crimes_df.show(5)\n",
    "\n",
    "# keep only first duplicate value in offence_code_df\n",
    "offense_codes_df = offense_codes_df.dropDuplicates(['CODE'])\n",
    "offense_codes_df.sort ('CODE').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (window function) Partition by district , order by  year and then count the offenses including rolling count for each district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------------+-----------------------+\n",
      "|DISTRICT|YEAR|count_incidents|rolling_count_incidents|\n",
      "+--------+----+---------------+-----------------------+\n",
      "|      A1|2015|           6015|                   6015|\n",
      "|      A1|2016|          10923|                  16938|\n",
      "|      A1|2017|          11375|                  28313|\n",
      "|      A1|2018|           7404|                  35717|\n",
      "|     A15|2015|           1027|                   1027|\n",
      "|     A15|2016|           1986|                   3013|\n",
      "|     A15|2017|           2167|                   5180|\n",
      "|     A15|2018|           1325|                   6505|\n",
      "|      A7|2015|           2426|                   2426|\n",
      "|      A7|2016|           4130|                   6556|\n",
      "|      A7|2017|           4264|                  10820|\n",
      "|      A7|2018|           2724|                  13544|\n",
      "|      B2|2015|           8687|                   8687|\n",
      "|      B2|2016|          15706|                  24393|\n",
      "|      B2|2017|          15680|                  40073|\n",
      "|      B2|2018|           9872|                  49945|\n",
      "|      B3|2015|           5617|                   5617|\n",
      "|      B3|2016|          11145|                  16762|\n",
      "|      B3|2017|          11195|                  27957|\n",
      "|      B3|2018|           7485|                  35442|\n",
      "+--------+----+---------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1. (window function) Partition by district , order by  year and then rolling count the offenses\n",
    "\n",
    "# create new column year  incuding  onlu INCIDENT_NUMBER , DISTRICT , YEAR\n",
    "window_df = crimes_df.select('INCIDENT_NUMBER', 'DISTRICT', 'YEAR')\n",
    "\n",
    "# create window partition by DISTRICT and order by YEAR\n",
    "window = Window.partitionBy('DISTRICT').orderBy('YEAR')\n",
    "\n",
    "# remove null values from DISTRICT column\n",
    "window_df = window_df.filter(F.col('DISTRICT').isNotNull())\n",
    "\n",
    "#  column count_incidents\n",
    "window_df = window_df.groupBy('DISTRICT', 'YEAR').agg(F.count('INCIDENT_NUMBER').alias('count_incidents'))\n",
    "\n",
    "# rolling_count_incidents\n",
    "window_df = window_df.withColumn('rolling_count_incidents', F.sum('count_incidents').over(window))\n",
    "\n",
    "window_df.show()\n",
    "\n",
    "\n",
    "################### SAVE to POSTGRES #######################\n",
    "window_df.write.jdbc(url=URL, table='rolling_count', mode='overwrite', properties=Properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spark_project_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c56a022bae3e6673f64e7513c309b6c71ec5c3972dcbf7b2ef6b0a8ca8eda9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
